{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NanoFore\n",
    "\n",
    "\n",
    "## STR detection\n",
    "\n",
    "The STR profile is generated using two approaches; one based on the length of the STR regions the other on the  actual sequence.\n",
    "\n",
    "- Part 1: Splicing the reads into subreads \n",
    "- Part 2A: Length based profiling\n",
    "- Part 2B: Sequence based profiling \n",
    "\n",
    "For all below replace '/media/sf_vm_shared/nanopore' with the root path to the project directory.\n",
    "In my case, the fast5 files are in a subdirectory 'pass', results will be written into a subdirectory 'results' and any other files needed are located in the root of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Splicing read into subreads. \n",
    "\n",
    "- Loading Primers\n",
    "- Loading Sequencing data\n",
    "- Using fuzzy Regex to identify the primers and slice the reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Checking current fast5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyses (group)\n",
      "Analyses/Basecall_1D_000 (group)\n",
      "Analyses/Basecall_1D_000/BaseCalled_complement (group)\n",
      "Analyses/Basecall_1D_000/BaseCalled_complement/Events \n",
      "Analyses/Basecall_1D_000/BaseCalled_complement/Fastq \n",
      "Analyses/Basecall_1D_000/BaseCalled_complement/Model \n",
      "Analyses/Basecall_1D_000/BaseCalled_template (group)\n",
      "Analyses/Basecall_1D_000/BaseCalled_template/Events \n",
      "Analyses/Basecall_1D_000/BaseCalled_template/Fastq \n",
      "Analyses/Basecall_1D_000/BaseCalled_template/Model \n",
      "Analyses/Basecall_1D_000/Configuration (group)\n",
      "Analyses/Basecall_1D_000/Configuration/aggregator (group)\n",
      "Analyses/Basecall_1D_000/Configuration/basecall_1d (group)\n",
      "Analyses/Basecall_1D_000/Configuration/basecall_2d (group)\n",
      "Analyses/Basecall_1D_000/Configuration/calibration_strand (group)\n",
      "Analyses/Basecall_1D_000/Configuration/components (group)\n",
      "Analyses/Basecall_1D_000/Configuration/general (group)\n",
      "Analyses/Basecall_1D_000/Configuration/hairpin_align (group)\n",
      "Analyses/Basecall_1D_000/Configuration/post_processing (group)\n",
      "Analyses/Basecall_1D_000/Configuration/post_processing.3000Hz (group)\n",
      "Analyses/Basecall_1D_000/Configuration/split_hairpin (group)\n",
      "Analyses/Basecall_1D_000/Log \n",
      "Analyses/Basecall_1D_000/Summary (group)\n",
      "Analyses/Basecall_1D_000/Summary/basecall_1d_complement (group)\n",
      "Analyses/Basecall_1D_000/Summary/basecall_1d_template (group)\n",
      "Analyses/Basecall_2D_000 (group)\n",
      "Analyses/Basecall_2D_000/BaseCalled_2D (group)\n",
      "Analyses/Basecall_2D_000/BaseCalled_2D/Alignment \n",
      "Analyses/Basecall_2D_000/BaseCalled_2D/Fastq \n",
      "Analyses/Basecall_2D_000/Configuration (group)\n",
      "Analyses/Basecall_2D_000/Configuration/aggregator (group)\n",
      "Analyses/Basecall_2D_000/Configuration/basecall_1d (group)\n",
      "Analyses/Basecall_2D_000/Configuration/basecall_2d (group)\n",
      "Analyses/Basecall_2D_000/Configuration/calibration_strand (group)\n",
      "Analyses/Basecall_2D_000/Configuration/components (group)\n",
      "Analyses/Basecall_2D_000/Configuration/general (group)\n",
      "Analyses/Basecall_2D_000/Configuration/hairpin_align (group)\n",
      "Analyses/Basecall_2D_000/Configuration/post_processing (group)\n",
      "Analyses/Basecall_2D_000/Configuration/post_processing.3000Hz (group)\n",
      "Analyses/Basecall_2D_000/Configuration/split_hairpin (group)\n",
      "Analyses/Basecall_2D_000/HairpinAlign (group)\n",
      "Analyses/Basecall_2D_000/HairpinAlign/Alignment \n",
      "Analyses/Basecall_2D_000/Log \n",
      "Analyses/Basecall_2D_000/Summary (group)\n",
      "Analyses/Basecall_2D_000/Summary/basecall_2d (group)\n",
      "Analyses/Basecall_2D_000/Summary/hairpin_align (group)\n",
      "Analyses/Basecall_2D_000/Summary/post_process_complement (group)\n",
      "Analyses/Basecall_2D_000/Summary/post_process_template (group)\n",
      "Analyses/Calibration_Strand_000 (group)\n",
      "Analyses/Calibration_Strand_000/Configuration (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/aggregator (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/basecall_1d (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/basecall_2d (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/calibration_strand (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/components (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/general (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/hairpin_align (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/post_processing (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/post_processing.3000Hz (group)\n",
      "Analyses/Calibration_Strand_000/Configuration/split_hairpin (group)\n",
      "Analyses/Calibration_Strand_000/Log \n",
      "Analyses/Calibration_Strand_000/Summary (group)\n",
      "Analyses/EventDetection_000 (group)\n",
      "Analyses/EventDetection_000/Configuration (group)\n",
      "Analyses/EventDetection_000/Configuration/abasic_detection (group)\n",
      "Analyses/EventDetection_000/Configuration/event_detection (group)\n",
      "Analyses/EventDetection_000/Configuration/hairpin_detection (group)\n",
      "Analyses/EventDetection_000/Reads (group)\n",
      "Analyses/EventDetection_000/Reads/Read_1000 (group)\n",
      "Analyses/EventDetection_000/Reads/Read_1000/Events \n",
      "Analyses/Hairpin_Split_000 (group)\n",
      "Analyses/Hairpin_Split_000/Configuration (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/aggregator (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/basecall_1d (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/basecall_2d (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/calibration_strand (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/components (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/general (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/hairpin_align (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/post_processing (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/post_processing.3000Hz (group)\n",
      "Analyses/Hairpin_Split_000/Configuration/split_hairpin (group)\n",
      "Analyses/Hairpin_Split_000/Log \n",
      "Analyses/Hairpin_Split_000/Summary (group)\n",
      "Analyses/Hairpin_Split_000/Summary/split_hairpin (group)\n",
      "UniqueGlobalKey (group)\n",
      "UniqueGlobalKey/channel_id (group)\n",
      "UniqueGlobalKey/context_tags (group)\n",
      "UniqueGlobalKey/tracking_id (group)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Example f5 file\n",
    "f5 = h5py.File('/media/genomics/raw_data/FW01C250_20160531_FN_MN15671_sequencing_run_SNPforID_310516_26276_ch101_read1000_strand.fast5')\n",
    "\n",
    "# Walk the file structure\n",
    "def printname(name):\n",
    "  print(name,'(group)' if type(f5[name]) == h5py._hl.group.Group else '')\n",
    "\n",
    "f5.visit(printname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extracting Fastq file from Fast5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fast5 files without called fastq sequence\n"
     ]
    }
   ],
   "source": [
    "import h5py, glob\n",
    "\n",
    "infiles          = glob.glob(\"/media/genomics/raw_data/*fast5\")\n",
    "outfile          = \"/media/genomics/raw_data/Nanopore_data/ligatedSTR.fastq\"\n",
    "outfile          = open(outfile, 'wt')\n",
    "withoutFQcounter = 0\n",
    "\n",
    "for f5file in infiles:\n",
    "  f5 = h5py.File(f5file)\n",
    "  try:\n",
    "    fq = f5[\"Analyses/Basecall_2D_000/BaseCalled_2D/Fastq\"]\n",
    "    outfile.write(fq.value.decode())\n",
    "  except KeyError:\n",
    "    withoutFQcounter += 1 \n",
    "  f5.close()\n",
    "           \n",
    "print(withoutFQcounter, \"fast5 files without called fastq sequence\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Counting primers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append('/media/genomics/raw_data/')\n",
    "from MyFLq import complement\n",
    "\n",
    "fastq = open(\"/media/genomics/raw_data/Nanopore_data/ligatedSTR.fastq\")\n",
    "c     = count(0)\n",
    "reads = [line for line in fastq if next(c)%4 == 1]\n",
    "\n",
    "# Plot length histogram\n",
    "plt.hist([len(r) for r in reads], bins=1000)\n",
    "plt.xlabel(\"Length (Bp)\",size=33)\n",
    "plt.ylabel(\"Frequency (n)\",size=33)\n",
    "plt.savefig('myfig.svg')\n",
    "\n",
    "\n",
    "allDataOneString = ' '.join(reads)\n",
    "\n",
    "# Primers\n",
    "primers = {}\n",
    "for line in open(\"/media/genomics/raw_data/primers2.csv\"):\n",
    "  if line.startswith('#'): continue\n",
    "  line             = line.strip().split(',')\n",
    "  primers[line[0]] = {'for':  [line[1],allDataOneString.count(line[1])],\n",
    "                      'forc': [complement(line[1]),allDataOneString.count(complement(line[1]))],\n",
    "                      'rev':  [line[2],allDataOneString.count(line[2])],\n",
    "                      'revc': [complement(line[2]),allDataOneString.count(complement(line[2]))],\n",
    "                     }\n",
    "\n",
    "for p in primers:\n",
    "  print(p,\n",
    "        primers[p]['for'][1],\n",
    "        primers[p]['forc'][1],\n",
    "        primers[p]['rev'][1],\n",
    "        primers[p]['revc'][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2A: Analyzing full reads and creating a length based profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyzing full reads\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import count\n",
    "from math import floor,ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import expanduser as eu\n",
    "import sys, os\n",
    "sys.path.append(eu(\"~/repos/myflq/src/\"))\n",
    "from MyFLq import complement, calculateAlleleNumber, Locus\n",
    "\n",
    "loci = Locus.makeLocusDict(('csv',eu('~/repos/Nanofore/lociConfiguration.csv')))\n",
    "\n",
    "exit()\n",
    "#Functions\n",
    "def bpTOfloat(bpLength,locusType):\n",
    "    return int(bpLength)+(int(str(bpLength).split('.')[1]) if '.' in str(bpLength) else 0)/locusType\n",
    "\n",
    "def floatTObp(floatLength,locusType):\n",
    "    return int(floatLength)+(int(str(floatLength).split('.')[1]) if '.' in str(floatLength) else 0)*locusType\n",
    "\n",
    "def calculateOriginalLength(repeats,locusType,refsize,refrepeats):\n",
    "    fullRefRepeats,partialRefRepeat = str(float(refrepeats)).split('.')\n",
    "    fullRepeats,partialRepeat = str(float(repeats)).split('.')\n",
    "    return refsize+locusType*(int(fullRepeats)-int(fullRefRepeats))+(int(partialRepeat)-int(partialRefRepeat))\n",
    "\n",
    "def gaus(x,a,x0,sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n",
    "\n",
    "def gaus2(x,a_1,x0_1,sigma_1,a_2,x0_2,sigma_2):\n",
    "    return a_1*np.exp(-(x-x0_1)**2/(2*sigma_1**2))+a_2*np.exp(-(x-x0_2)**2/(2*sigma_2**2))\n",
    "\n",
    "# Main class\n",
    "class LigatedRead:\n",
    "    \"\"\"\n",
    "    Class that represents a ligated read of subreads.\n",
    "    Methods allow to extract the subreads for further processing.\n",
    "    The initial read can contain '&', which are divisions between\n",
    "    already known subreads.\n",
    "    \"\"\"\n",
    "    def __init__(self, read, loci,minSize1read=70, maxSize1read=500, maxPrimerErrors=0):\n",
    "        self.read = read\n",
    "        for l in loci:\n",
    "            # Prep loci for use with MyFLq.calculateAlleleNumber\n",
    "            loci[l]['ref_length']       = len(loci[l]['ref_sequence'])\n",
    "            loci[l]['ref_alleleNumber'] = loci[l]['ref_number']\n",
    "            \n",
    "        self.loci                = loci\n",
    "        self.maxSize1read        = maxSize1read\n",
    "        self.minSize1read        = minSize1read\n",
    "        self.maxPrimerErrors     = maxPrimerErrors\n",
    "        self.averagePrimerLength = sum(len(self.loci[l]['ref_forwardP'])+\n",
    "                                       len(self.loci[l]['ref_reverseP'])\n",
    "                                       for l in self.loci)/(2*len(self.loci))\n",
    "        \n",
    "    def processPrimers(self,withPrimerErrors=0):\n",
    "        # Find previous readfragments\n",
    "        self.markpositions('&')\n",
    "        # Primer errors setup\n",
    "        if withPrimerErrors:\n",
    "            # Calculate kmer size\n",
    "            kmerSize = int(self.averagePrimerLength/(1+withPrimerErrors))  # k is half primer length\n",
    "            # Kmer count in expected sequences\n",
    "            self.reference_kmers = {}\n",
    "            for l in self.loci:\n",
    "                for seq in (self.loci[l]['ref_sequence'],complement(self.loci[l]['ref_sequence'])):\n",
    "                    for i in range(len(seq)+1-kmerSize):\n",
    "                        try: self.reference_kmers[seq[i:i+kmerSize]]+=1\n",
    "                        except KeyError: self.reference_kmers[seq[i:i+kmerSize]]=1\n",
    "        # Find primers\n",
    "        for locus in self.loci:\n",
    "            for primertype in ('ref_forwardP','ref_reverseP','ref_forwardP_c','ref_reverseP_c'):\n",
    "                primer = (self.loci[locus][primertype] if not primertype.endswith('_c')\n",
    "                          else complement(self.loci[locus][primertype[:-2]]))\n",
    "                if not withPrimerErrors:\n",
    "                    self.markpositions(primer,locus,primertype)\n",
    "                else:\n",
    "                    primerKmers = {i:primer[i:i+kmerSize] for i in range(0,len(primer)+1-kmerSize,kmerSize)}\n",
    "                    if len(primer)%kmerSize != 0: primerKmers[kmerSize] = primer[-kmerSize:]\n",
    "                    \n",
    "                    # (YG) DEBUG\n",
    "                    print('{} {} {} has {} k-mers (k={})'.format(locus, primertype, primer, len(primerKmers), kmerSize))\n",
    "                    \n",
    "                    for o in primerKmers:\n",
    "                        # (YG) kmer must occur only once in reference sequence (which includes the primer sequences)\n",
    "                        if self.reference_kmers[primerKmers[o]] == 1:\n",
    "                            self.markpositions(primerKmers[o],locus,primertype,offset=o)\n",
    "        # Remove duplicates TODO (make set, then sorted for list)\n",
    "        # Sort on position\n",
    "        self.primerPositions.sort(key = lambda x: x[0])\n",
    "\n",
    "    def markpositions(self,pattern,locus=None,primertype=None,offset=0):\n",
    "        try:\n",
    "            currentPosition = self.read.find(pattern)\n",
    "            while currentPosition != -1:\n",
    "                self.primerPositions.append((currentPosition-offset,locus,primertype))\n",
    "                currentPosition = self.read.find(pattern,currentPosition+1)\n",
    "            \n",
    "        except AttributeError:\n",
    "            self.primerPositions = []\n",
    "            self.markpositions(pattern,locus,primertype)\n",
    "\n",
    "    def extractReads(self,filterArtefacts=True):\n",
    "        self.subreads = []         # [((pos, locus, primertype), (pos, locus, primertype)),]\n",
    "        pp = self.primerPositions  # [(pos, locus, primertype), ]\n",
    "        for i in range(len(pp)-1):\n",
    "            if (self.minSize1read < (pp[i+1][0]-pp[i][0]) < self.maxSize1read and   # 70 < ampliconsize < 500\n",
    "                pp[i][1] == pp[i+1][1] and pp[i][2] and pp[i+1][2] and              # must be same locus, must have primertype\n",
    "                pp[i][2][:6] != pp[i+1][2][:6] and                                  # cannot be both ref_for or both ref_rev\n",
    "                (pp[i][2].endswith('_c') ^ pp[i+1][2].endswith('_c'))):             # one must end with _c\n",
    "                self.subreads.append((pp[i],pp[i+1]))\n",
    "        if filterArtefacts:\n",
    "            nonArtifacts   = {('ref_forwardP', 'ref_reverseP_c'),\n",
    "                              ('ref_reverseP', 'ref_forwardP_c')}\n",
    "            self.artifacts = [s for s in self.subreads if (s[0][2],s[1][2]) not in nonArtifacts]\n",
    "            self.subreads  = [s for s in self.subreads if (s[0][2],s[1][2]) in nonArtifacts]\n",
    "            \n",
    "            # (YG) DEBUG\n",
    "            #print('Found {} subreads, {} artifacts'.format(len(self.subreads), len(self.artifacts)))\n",
    "            #for artifact in self.artifacts:\n",
    "            #  print('Artifact: {}'.format(artifact))\n",
    "\n",
    "    def sortsubreads(self):\n",
    "        # Sort first on length\n",
    "        self.subreads.sort(key=lambda x: (x[1][0]-x[0][0])+len(self.loci[x[1][1]][x[1][2].replace('_c','')]))\n",
    "        # Then on locus\n",
    "        self.subreads.sort(key=lambda x: x[0][1])\n",
    "\n",
    "    def exportReads(self,filename,mode='wt',type='fasta',locus=None,alleleLength=None,maxReads=None):\n",
    "        ci = count(1)\n",
    "        with open(filename,mode) as outfile:\n",
    "            countBlankInSeq = 0\n",
    "            for r in self.subreads:\n",
    "                # (YG) sequence reconstruction: from startpos left primer match to startpos-1 right primer match + sequence of the right primer\n",
    "                seq = self.read[r[0][0]:r[1][0]+len(self.loci[r[1][1]][r[1][2].replace('_c','')])]\n",
    "                if ' ' in seq:\n",
    "                    countBlankInSeq+=1\n",
    "                    continue\n",
    "                if 'reverse' in r[0][2]:\n",
    "                    seq = complement(seq)\n",
    "                    orientation = 'reverse'\n",
    "                else: orientation = 'forward'\n",
    "                outfile.write('>{} {} ({}): {} - {} ({})\\n'.format(next(ci),r[0][1],len(seq),r[0][0],r[1][0],orientation))\n",
    "                outfile.write(seq+'\\n')\n",
    "            if countBlankInSeq: print(countBlankInSeq,\"reads contained blanks and were not exported\")\n",
    "\n",
    "    def histLengths(self):\n",
    "        self.sortsubreads()\n",
    "        self.histLengthData = {}\n",
    "        for subr in self.subreads:\n",
    "            locus = subr[0][1]\n",
    "            # (YG) sequence reconstruction: from startpos left primer match to startpos-1 right primer match + sequence of the right primer\n",
    "            seq = self.read[subr[0][0]:subr[1][0]+len(self.loci[locus][subr[1][2].replace('_c','')])]\n",
    "            #if self.loci[locus]['locusType']:\n",
    "            #    length = float(calculateAlleleNumber(seq,self.loci[locus]))\n",
    "            #else:\n",
    "            length = len(seq)\n",
    "            try:\n",
    "                self.histLengthData[locus][length]+=1\n",
    "            except KeyError:\n",
    "                if locus not in self.histLengthData: self.histLengthData[locus] = {}\n",
    "                self.histLengthData[locus][length]=1\n",
    "\n",
    "    def peakDetection(self,peak_min_width=8,peak_max_width=12,\n",
    "                      peak_max_hight_diff=0.2,\n",
    "                      includeRefProfile=False):\n",
    "        from scipy import signal\n",
    "        from scipy.optimize import curve_fit\n",
    "        import numpy as np\n",
    "        \n",
    "        self.profile  = {}\n",
    "        plotDimension = np.sqrt(len(self.histLengthData))\n",
    "        fig,axes      = plt.subplots(ceil(plotDimension),ceil(len(self.histLengthData)/plotDimension),sharex=True,sharey=True)\n",
    "        fig.set_size_inches(2*11.692, 2*8.267)\n",
    "        \n",
    "        for l,ax in zip(sorted(self.histLengthData),\n",
    "                        (ax for row in axes for ax in row)):\n",
    "            locusType = self.loci[l]['locusType']\n",
    "            x         = sorted(self.histLengthData[l])\n",
    "            x_range   = list(range(0,x[-1]+1))\n",
    "            y         = [0 if i not in x else self.histLengthData[l][i] for i in x_range]\n",
    "            peaks     = signal.find_peaks_cwt(y,np.arange(peak_min_width,peak_max_width))\n",
    "            peaks     = [(x_range[p],y[p]) for p in peaks]\n",
    "            peaks.sort(key = lambda x: x[1],reverse = True)\n",
    "            \n",
    "            # Potential peaks\n",
    "            if len(peaks) > 1 and peaks[0][1]*peak_max_hight_diff < peaks[1][1]:\n",
    "                peaks = peaks[:2]\n",
    "            else:\n",
    "                peaks = [peaks[0]]\n",
    "                \n",
    "            # Calculate gaussian fit\n",
    "            x = np.array(x)\n",
    "            y = np.array([self.histLengthData[l][i] for i in x])\n",
    "            ax.plot(x,y,'b+',label=l, marker='.')\n",
    "            try:\n",
    "                if len(peaks) == 1:\n",
    "                    popt,pcov       = curve_fit(gaus,x,y,p0=[max(y),peaks[0][0],peak_min_width])\n",
    "                    self.profile[l] = ((calculateAlleleNumber(' '*int(round(popt[1])),self.loci[l]) if locusType\n",
    "                                        else round(popt[1]), popt[2]/(locusType if locusType else 1)),)\n",
    "                \n",
    "                    ax.plot(x,gaus(x,*popt),'ro:',label='fit1', marker='.')\n",
    "                elif len(peaks) == 2:\n",
    "                    popt,pcov = curve_fit(gaus2,x,y,p0=[max(y),peaks[0][0],peak_min_width,\n",
    "                                                        max(y),peaks[1][0],peak_min_width])\n",
    "                    self.profile[l] = ((calculateAlleleNumber(' '*int(round(popt[1])),self.loci[l]) if locusType\n",
    "                                        else round(popt[1]), popt[2]/(locusType if locusType else 1)),\n",
    "                                       (calculateAlleleNumber(' '*int(round(popt[4])),self.loci[l]) if locusType\n",
    "                                        else round(popt[4]), popt[5]/(locusType if locusType else 1)))\n",
    "                    ax.plot(x,gaus2(x,*popt),'ro:',label='fit2', marker='.')\n",
    "                else: self.profile[l] = None\n",
    "            except RuntimeError:\n",
    "                self.profile[l] = None\n",
    "                \n",
    "            if includeRefProfile and locusType:\n",
    "                # Mark the reference profile allele lengths with vertical green lines\n",
    "                for ra in self.referenceProfile[l]:\n",
    "                    position = calculateOriginalLength(ra, locusType,\n",
    "                                                       self.loci[l]['ref_length'],\n",
    "                                                       self.loci[l]['ref_number'])\n",
    "                    ax.plot((position,position), ax.get_ylim(),'g-')\n",
    "                    \n",
    "            ax.legend()\n",
    "        plt.show(block=False)\n",
    "\n",
    "    def CPI(self,populationFile):\n",
    "        # Reprocess self.profile to ranges of alleles\n",
    "        self.profileCI = {}\n",
    "        self.CPI_value = 1\n",
    "        self.populationData = pd.read_csv(populationFile)\n",
    "        self.CPI_unusedLoci = set(self.profile) - set(self.populationData['#Locus name'])        \n",
    "        \n",
    "        for l in self.profile:\n",
    "            if l in self.CPI_unusedLoci: continue\n",
    "            locusAlleles = self.populationData[self.populationData[\n",
    "                self.populationData.columns[0]]==l]\n",
    "            locusAN = locusAlleles[\"Allele number\"]\n",
    "            # TODO float/allele number issue\n",
    "            alleleRanges = [(float(g[0])-g[1],float(g[0])+g[1]) for g in self.profile[l]]\n",
    "            if len(alleleRanges) == 2:\n",
    "                alleleRanges.sort(key = lambda x: x[0])\n",
    "                if alleleRanges[0][1] > alleleRanges[1][0]:\n",
    "                    mean = (alleleRanges[0][1] + alleleRanges[1][0])/2\n",
    "                    alleleRanges = [(alleleRanges[0][0],mean),(mean,alleleRanges[1][1])]\n",
    "                    # TODO check if with 'mean' is best strategy\n",
    "            self.profileCI[l] = alleleRanges\n",
    "            af1 = locusAlleles[(locusAN >= alleleRanges[0][0]) &\n",
    "                               (locusAN < alleleRanges[0][1])][\"Allele Frequency\"].sum()\n",
    "            if len(alleleRanges) == 1:\n",
    "                locusProbability = af1**2\n",
    "            else:\n",
    "                af2 = locusAlleles[(locusAN >= alleleRanges[1][0]) &\n",
    "                                   (locusAN < alleleRanges[1][1])][\"Allele Frequency\"].sum()\n",
    "                locusProbability = 2*af1*af2\n",
    "            self.CPI_value*=locusProbability\n",
    "\n",
    "    def linkReferenceProfile(self,referenceProfileFile):\n",
    "        self.referenceProfile = {}\n",
    "        with open(referenceProfileFile) as inprofile:\n",
    "            for line in inprofile:\n",
    "                if line.startswith('#'): continue\n",
    "                line = line.strip().split(',')\n",
    "                self.referenceProfile[line[0]] = (float(line[1]),float(line[2]))\n",
    "                \n",
    "# Processing reads\n",
    "fastq    = open(\"/media/genomics/raw_data/Nanopore_data/ligatedSTR.fasta\")\n",
    "c        = count(0)\n",
    "reads    = [line for line in fastq if next(c)%2 == 1]\n",
    "allreads = '&'.join(reads)\n",
    "\n",
    "#ligread = LigatedRead('&'.join(reads),loci)\n",
    "ligread = LigatedRead(allreads,loci)\n",
    "#ligread.linkReferenceProfile('/media/sf_vm_shared/nanopore/Profile9948A')\n",
    "ligread.linkReferenceProfile(eu('~/repos/Nanofore/Profile9948A'))\n",
    "ligread.processPrimers(withPrimerErrors=1)\n",
    "ligread.extractReads()\n",
    "ligread.histLengths()\n",
    "ligread.exportReads('/Nanopore_data/ligatedSTR.fastq/STR.fasta')\n",
    "ligread.peakDetection(includeRefProfile=True)\n",
    "ligread.CPI(populationFile = '/home/senne/nanopore/SNP/Europe_SNP_freq.csv')\n",
    "print(\"RPM value profile:\", ligread.CPI_value)\n",
    "\n",
    "# (YG) Display profile\n",
    "print()\n",
    "print('Estimated profile:')\n",
    "for locus in sorted(ligread.profile):\n",
    "    alleles = ':'.join([str(a[0]) for a in ligread.profile[locus]])\n",
    "    print('  {} {}'.format(locus, alleles))\n",
    "\n",
    "# Calculate an original length\n",
    "#calculateOriginalLength(10.3,\n",
    "                       # ligread.loci['D13S317']['locusType'],\n",
    "                        #ligread.loci['D13S317']['ref_length'],\n",
    "                        #ligread.loci['D13S317']['ref_number'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##PART 2B: Sequence based profiling \n",
    "\n",
    "- Mapping using BWA and SAM\n",
    "- Extracting the uniquely mapped and once with a mapping score 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init\n",
    "\n",
    "strFile   = '/media/genomics/raw_data/STR_database.fasta' \n",
    "readFile  = '/media/genomics/raw_data/Nanopore_data/ligatedSTR.fasta\n",
    "samfile = '/media/genomics/raw_data/STR.sam'\n",
    "bamfile = '/media/genomics/raw_data/STR.bam'.\n",
    "bambaifile = '/media/genomics/raw_data/STR.bam.bai'\n",
    "txtfile = '/media/genomics/raw_data/STR.txt'\n",
    "\n",
    "bwa       = '/opt/tools/bwa-0.7.15'            # v0.7.5\n",
    "samtools  = '/opt/tools/samtools-1.3.1'        # v1.3.1\n",
    "bcftools  = '/opt/tools/bcftools-1.3.1'        # v1.3.1\n",
    "\n",
    "\n",
    "# Check\n",
    "!ls {strFile}\n",
    "print('Number of STRs in reference file:')\n",
    "!grep -c \">\" {strFile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build index of the references\n",
    "!{bwa} index {strFile}\n",
    "\n",
    "# Map reads\n",
    "!{bwa} mem -x ont2d {strFile} {readFile} > {samfile}\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sorted bam and index\n",
    "!{samtools} view -Sbu {samfile} | {samtools} sort -o {bamfile} -\n",
    "!{samtools} index {bamfile} {bambaifile}\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the uniquely mapped and once with a mapping score 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd /home/senne/nanopore/Multiplex/results_26-06-2017/Basecalling_2/STR/\n",
    "echo -n \"\" > STR.txt\n",
    "for i in \"Amelogenine_X\" \"Amelogenine_Y\" \"D3S1358_13\" \"D3S1358_14\" \"D3S1358_15\" \"D3S1358_16\" \"D3S1358_17\" \"D3S1358_18\" \"D3S1358_19\" \"D21S11_27\" \"D21S11_28\"  \"D21S11_29\" \"D21S11_30\" \"D21S11_31\" \"D21S11_31.2\" \"D21S11_32\" \"D21S11_32.2\" \"D21S11_33.2\" \"TH01_5\" \"TH01_6\" \"TH01_7\" \"TH01_8\" \"TH01_9\" \"TH01_9.3\" \"TH01_10\" \"TH01_11\" \"D16S539_9\" \"D16S539_10\" \"D16S539_11\" \"D16S539_12\" \"D16S539_13\" \"D16S539_14\" \"D16S539_15\" \"D7S820_7\" \"D7S820_8\" \"D7S820_9\" \"D7S820_10\" \"D7S820_11\" \"D7S820_12\" \"D7S820_13\" \"D7S820_14\" \"D13S317_8\" \"D13S317_9\" \"D13S317_10\" \"D13S317_11\" \"D13S317_12\" \"D13S317_13\" \"D13S317_14\" \"D13S818_15\" \"D13S818_16\" \"D18S51_12\" \"D18S51_13\" \"D18S51_14\" \"D18S51_15\" \"D18S51_16\" \"D18S51_17\" \"D18S51_18\" \"D18S51_19\" \"D18S51_20\" \"D5S818_9\" \"D5S818_10\" \"D5S818_11\" \"D5S818_12\" \"D5S818_13\" \"D5S818_14\" \"D5S818_15\" \"D5S818_16\" \"D5S818_17\" \"D5S818_18\" \"D8S1179_8\" \"D8S1179_9\" \"D8S1179_10\" \"D8S1179_12\" \"D8S1179_13\" \"D8S1179_14\" \"D8S1179_15\" \"D8S1179_16\" \"D8S1179_17\" \"D8S1179_18\" \"FGA_18\" \"FGA_19\" \"FGA_20\" \"FGA_21\" \"FGA_22\" \"FGA_23\" \"FGA_24\" \"FGA_25\" \"FGA_26\" \"FGA_27\" \"FGA_28\" \"TPOX_6\" \"TPOX_7\" \"TPOX_8\" \"TPOX_9\" \"TPOX_10\" \"TPOX_11\" \"TPOX_12\" \"TPOX_13\" \"vWA_15\" \"vWA_16\" \"vWA_17\" \"vWA_18\" \"vWA_19\" \"vWA_20\" \"SE33_15\" \"SE33_16\" \"SE33_18\" \"SE33_19\" \"SE33_20\" \"SE33_21\" \"SE33_21.2\"  \"SE33_22.2\" \"SE33_23.2\" \"SE33_24.2\" \"SE33_25.2\" \"SE33_26.2\" \"SE33_27.2\" \"SE33_28.2\" \"SE33_29.2\" \"SE33_30.2\" \"SE33_31.2\"; do \n",
    "    echo -n \"$i;\" >> STR.txt\n",
    "    awk '$2 < 1' STR.sam | awk '$5 > 0'| cut -f3 -d$'\\t'| grep -c $i >> STR-NB04.txt\n",
    "done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
